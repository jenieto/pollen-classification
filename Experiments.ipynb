{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMy/Ug0LcRCWBduXsR2+7nY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenieto/pollen-classification/blob/master/Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPnv6Jz4glpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Montamos Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bSgNbXQiQgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract data\n",
        "!mkdir -p /data\n",
        "!tar xvzf \"/content/drive/My Drive/Datasets/anuka1200.tar.gz\" --directory /data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FN1wGiqjiyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets\n",
        "import tensorflow as tf\n",
        "\n",
        "training_size = 0.8\n",
        "validation_size = 0.2\n",
        "image_height = 50\n",
        "image_width = 50\n",
        "image_channels = 3\n",
        "image_size = (image_height, image_width, image_channels)\n",
        "directory = '/data/anuka1200'\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=directory,\n",
        "    subset=\"training\",\n",
        "    validation_split=training_size,\n",
        "    seed=123,\n",
        "    image_size=(image_height, image_width))\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=directory,\n",
        "    subset=\"validation\",\n",
        "    validation_split=validation_size,\n",
        "    seed=123,\n",
        "    image_size=(image_height, image_width))\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "print('Class Names', class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rguRs6QIl_sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explore data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lCTfIYMma-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def create_models():\n",
        "  model_0 = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=image_size),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(2, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model_1 = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=image_size),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(2, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model_2 = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=image_size),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(2, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  return {\n",
        "      'model_0': model_0,\n",
        "      'model_1': model_1,\n",
        "      'model_2': model_2\n",
        "  } "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymfBwW_Cnzry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Compile models\n",
        "def compile_models(models_dict):\n",
        "  for model_name, model in models_dict.items():\n",
        "    model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy']) \n",
        "    # print(f'{model_name} summary:')\n",
        "    # model.summary()\n",
        "\n",
        "  \n",
        "def model_callbacks(model_name):\n",
        "  filepath_mdl = 'model.h5'\n",
        "  checkpoint = ModelCheckpoint(filepath_mdl, monitor='val_loss', verbose=1, save_best_only=True) # Va guardando los pesos tras cada época\n",
        "  log_dir = f\"logs/fit/{model_name}/\"# + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True) # Para graficado de las estadísticas durante el entrenamiento\n",
        "  earlystopping = EarlyStopping(patience=20, verbose=1) # Detiene el entrenamiento prematuramente si validation accuracy lleva sin aumentar varias épocas\n",
        "  return [checkpoint, tensorboard, earlystopping]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGQ_zZQ7n9UG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train models\n",
        "def train_models(models_dict):\n",
        "  epochs=100\n",
        "  for model_name, model in models_dict.items():\n",
        "    print('-----------------------------------------------------------------------')\n",
        "    print(f'Fitting model {model_name}')\n",
        "    history = model.fit(\n",
        "      train_dataset,\n",
        "      validation_data=validation_dataset,\n",
        "      epochs=epochs,\n",
        "      callbacks=model_callbacks(model_name))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KXcPwqYgjNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute\n",
        "models = create_models()\n",
        "compile_models(models)\n",
        "train_models(models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ggZGlDgcHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}