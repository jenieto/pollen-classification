{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiments.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXkVfdvcDq0wo+6Lv074sE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenieto/pollen-classification/blob/master/Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPnv6Jz4glpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Montamos Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bSgNbXQiQgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract data\n",
        "!mkdir -p /data\n",
        "!tar xvzf \"/content/drive/My Drive/Datasets/anuka1200.tar.gz\" --directory /data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FN1wGiqjiyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create datasets\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "training_size = 0.8\n",
        "validation_size = 0.2\n",
        "image_height = 96\n",
        "image_width = 96\n",
        "image_channels = 1\n",
        "image_size = (image_height, image_width, image_channels)\n",
        "directory = '/data/anuka1200'\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=directory,\n",
        "    subset='training',\n",
        "    labels='inferred',\n",
        "    validation_split=validation_size,\n",
        "    seed=123,\n",
        "    color_mode='grayscale',\n",
        "    image_size=(image_height, image_width))\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=directory,\n",
        "    subset='validation',\n",
        "    labels='inferred',\n",
        "    validation_split=validation_size,\n",
        "    seed=123,\n",
        "    color_mode='grayscale',\n",
        "    image_size=(image_height, image_width))\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "print('Class Names', class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rguRs6QIl_sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explore data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    image = np.array(images[i].numpy().astype(\"uint8\"))\n",
        "    if image_channels == 1:\n",
        "      plt.imshow(image.squeeze(), cmap='gray')\n",
        "    else:\n",
        "      plt.imshow(image)\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lCTfIYMma-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "\n",
        "def create_models():\n",
        "  model_0 = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=image_size),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model_1 = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=image_size),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model_2 = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=image_size),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    # layers.Dropout(0.25),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    #layers.Dropout(0.25),\n",
        "    layers.Dense(128, activation='relu'), #, kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  model_3 = Sequential([\n",
        "    layers.experimental.preprocessing.Rescaling(1./255, input_shape=image_size),\n",
        "    layers.Conv2D(16, 3, activation='relu'),\n",
        "    # layers.Dropout(0.25),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    # layers.Dropout(0.25),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  '''mobile_net = MobileNetV2(weights='imagenet', include_top=False, input_shape=image_size, pooling='avg') #Load the MobileNet v2 model\n",
        "  mobile_net.trainable = False\n",
        "  model_4 = tf.keras.models.Sequential([\n",
        "    mobile_net,\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "  ])'''\n",
        "\n",
        "  return {\n",
        "      'model_0': model_0,\n",
        "      'model_1': model_1,\n",
        "      'model_2': model_2,\n",
        "      'model_3': model_3,\n",
        "      #'model_4': model_4\n",
        "  } "
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymfBwW_Cnzry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from datetime import datetime\n",
        "from IPython.display import Image\n",
        "\n",
        "def display_model(model_name, model):\n",
        "  plot_file = f'{model_name}.png'\n",
        "  plot_model(model, to_file=plot_file, show_shapes=True, show_layer_names=True) \n",
        "  Image(retina=True, filename=plot_file)\n",
        "\n",
        "# Compile models\n",
        "def compile_models(models_dict):\n",
        "  for model_name, model in models_dict.items():\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='  y',#tf.keras.losses.categorical_crossentropy(),\n",
        "              metrics=['binary_accuracy']) \n",
        "    display_model(model_name, model)\n",
        "    #print(f'{model_name} summary:')\n",
        "    #model.summary()\n",
        "\n",
        "  \n",
        "def model_callbacks(model_name):\n",
        "  filepath_mdl = f'{model_name}.h5'\n",
        "  checkpoint = ModelCheckpoint(filepath_mdl, monitor='val_loss', verbose=1, save_best_only=True) # Va guardando los pesos tras cada época\n",
        "  log_dir = f\"logs/fit/{model_name}/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard = TensorBoard(log_dir=log_dir, write_graph=True, write_images=True) # Para graficado de las estadísticas durante el entrenamiento\n",
        "  earlystopping = EarlyStopping(patience=10, verbose=1) # Detiene el entrenamiento prematuramente si validation accuracy lleva sin aumentar varias épocas\n",
        "  reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
        "  return [checkpoint, tensorboard, earlystopping, reduce_lr_loss]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGQ_zZQ7n9UG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train models\n",
        "def train_models(models_dict):\n",
        "  epochs=50\n",
        "  model_results = {}\n",
        "  for model_name, model in models_dict.items():\n",
        "    print('-----------------------------------------------------------------------')\n",
        "    print(f'Fitting model {model_name}')\n",
        "    history = model.fit(\n",
        "      train_dataset,\n",
        "      validation_data=validation_dataset,\n",
        "      epochs=epochs,\n",
        "      callbacks=model_callbacks(model_name))\n",
        "    model.load_weights(f'{model_name}.h5')\n",
        "    score = model.evaluate(validation_dataset)\n",
        "    model_results[model_name] = score\n",
        "  return model_results\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KXcPwqYgjNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execute\n",
        "models = create_models()\n",
        "compile_models(models)\n",
        "# results = train_models(models)\n",
        "\n",
        "print('------------ Results -----------------')\n",
        "for model_name, score in results.items():\n",
        "  print(f'{model_name}: validation_loss: {score[0]}, validation_accuracy:{score[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ggZGlDgcHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}